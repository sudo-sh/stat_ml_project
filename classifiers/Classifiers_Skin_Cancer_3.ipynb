{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classifiers_Skin_Cancer_3.ipynb","provenance":[],"mount_file_id":"1Sd7Uvd36MfvD9SSoB6ccnj4Q6TTTI3LC","authorship_tag":"ABX9TyMDHnVvzJJbxRyYFDgPEL87"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l2kd-ya5h2TZ","executionInfo":{"status":"ok","timestamp":1651860290835,"user_tz":240,"elapsed":8559,"user":{"displayName":"Mark Lee","userId":"14543298645444410514"}},"outputId":"fe6f67ee-b7c2-4f01-827d-f26227a3ba93"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting memory_profiler\n","  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n","Building wheels for collected packages: memory-profiler\n","  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=88e8ffa95070217c3a00f3cd53bccb998fd0758eb7f06775001bcafdac038ff2\n","  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n","Successfully built memory-profiler\n","Installing collected packages: memory-profiler\n","Successfully installed memory-profiler-0.60.0\n"]}],"source":["!pip install -U memory_profiler"]},{"cell_type":"code","source":["#Import all necessary libraries\n","import time\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn import metrics\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import svm #SVM Classifier\n","from sklearn import neighbors #KNN Classifier\n","from memory_profiler import profile #For memory usage profiling\n","from sklearn.metrics import confusion_matrix #For specificity and sensitivity metrics\n","from sklearn.neural_network import MLPClassifier #Multi-Layer Perceptron Classifier\n","from sklearn.naive_bayes import GaussianNB #Gaussian Naive Bayes Classifier\n","from sklearn.linear_model import LogisticRegression #Logistic Regression Classifier\n","from sklearn.preprocessing import StandardScaler #Used to standardize data (0 mean and unit variance)\n","from sklearn.preprocessing import MinMaxScaler #Used to normalize data between 0 and 1\n","from sklearn.model_selection import train_test_split #To split data into various validation/training/test sets\n","\n","#Ignore those pesky warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","#Load in data from files\n","dir_path = 'drive/MyDrive/Georgia Tech/Classes/Spring 2022/ECE 6254: Statistical Machine Learning/Final Project/skin_cancer_dataset_3/'\n","x_train = np.load(dir_path + 'X_train_resize.npy')\n","y_train = np.load(dir_path + 'y_train_resize.npy')\n","x_test = np.load(dir_path + 'X_test_resize.npy')\n","y_test = np.load(dir_path + 'y_test_resize.npy')\n","\n","#Flatten images into 2d arrays\n","x_train = x_train.flatten().reshape(2637, 1875)\n","x_test = x_test.flatten().reshape(660, 1875)\n","\n","#Split the training set into separate training set and validation set\n","x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.20, random_state=0)\n","\n","#Check their shape\n","print(\"x_train = \" + str(x_train.shape))\n","print(\"x_val = \" + str(x_val.shape))\n","print(\"x_test = \" + str(x_test.shape))\n","print(\"y_train = \" + str(y_train.shape))\n","print(\"y_val = \" + str(y_val.shape))\n","print(\"y_test = \" + str(y_test.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jteblVKYjCJV","executionInfo":{"status":"ok","timestamp":1651861376356,"user_tz":240,"elapsed":142,"user":{"displayName":"Mark Lee","userId":"14543298645444410514"}},"outputId":"9a8c0ea2-127a-45fc-80d3-bfe427d7722f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x_train = (2109, 1875)\n","x_val = (528, 1875)\n","x_test = (660, 1875)\n","y_train = (2109,)\n","y_val = (528,)\n","y_test = (660,)\n"]}]},{"cell_type":"code","source":["#Perform preprocessing of data for certain algorithms\n","\n","#Scale the data to 0 mean and unit variance\n","standard = StandardScaler()\n","x_train_standard = standard.fit_transform(x_train)\n","x_val_standard = standard.transform(x_val)\n","x_test_standard = standard.transform(x_test)\n","\n","#Normalize data to be between 0 and 1 since some algorithms (KNN) use Euclidean distance as a metric\n","normal = MinMaxScaler(feature_range=(0, 1))\n","x_train_normal = normal.fit_transform(x_train)\n","x_val_normal = normal.transform(x_val)\n","x_test_normal = normal.transform(x_test)\n","\n","#Combine training and validation sets for algorithms which don't require parameter tuning\n","x_val_train = np.concatenate((x_val, x_train))\n","y_val_train = np.concatenate((y_val, y_train))\n","x_val_train_standard = standard.transform(x_val_train)\n","x_val_train_normal = normal.transform(x_val_train)"],"metadata":{"id":"qXNgJ88hnFgJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train the KNN Classifier\n","def KNN_Classifier():\n","\n","  print(\"Training the KNN Classifier!\\n\")\n","\n","  #Declare variables used for k parameter sweep\n","  k_optimal = 1\n","  k_range = 100\n","  accuracy_KNN = 0.0\n","  accuracy_val = 0.0\n","\n","  #Train KNN Classifier and find k which yields maximum accuracy\n","  for k in range(1, k_range+1):\n","    KNN_clf = neighbors.KNeighborsClassifier(n_neighbors=k, weights='uniform')\n","    KNN_clf.fit(x_train, y_train)\n","    accuracy = KNN_clf.score(x_val, y_val)\n","    #print(\"k = \" + str(k) + \", Accuracy = \" + str(accuracy))\n","\n","    if accuracy > accuracy_val:\n","      k_optimal = k\n","      accuracy_val = accuracy\n","\n","  KNN_clf = neighbors.KNeighborsClassifier(n_neighbors=k_optimal, weights='uniform')\n","  KNN_clf.fit(x_train, y_train)\n","  train_accuracy = KNN_clf.score(x_train, y_train)\n","\n","  #Find the average runtime on the testing set\n","  num_runs = 20\n","  avg_time_KNN = 0.0\n","  for i in range(0, num_runs):\n","    start_time = time.time()\n","    accuracy_KNN = KNN_clf.score(x_test, y_test)\n","    avg_time_KNN += time.time() - start_time\n","  avg_time_KNN /= num_runs\n","  avg_time_KNN = str(round(avg_time_KNN, 4))\n","\n","  #Calculate sensitivity and specificity metrics\n","  y_pred_KNN = KNN_clf.predict(x_test)\n","  tn, fp, fn, tp = confusion_matrix(y_test, y_pred_KNN).ravel()\n","  specificity_KNN = tn / (tn+fp)\n","  sensitivity_KNN = tp / (tp+fn)\n","\n","  print(\"KNN Classifier Accuracy on Training Set = \" + str(train_accuracy))\n","  print(\"KNN Classifier Accuracy on Test Set = \" + str(accuracy_KNN))\n","  print(\"KNN Classifier Sensitivity on Test Set = \" + str(sensitivity_KNN))\n","  print(\"KNN Classifier Specificity on Test Set = \" + str(specificity_KNN))\n","  print(\"Optimal value of k = \" + str(k_optimal))\n","  print(\"Average Runtime of KNN Classifer on Test Set = \" + str(avg_time_KNN) + \" seconds\")\n","\n","KNN_Classifier()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UbsCeEqdnLod","executionInfo":{"status":"ok","timestamp":1651861447683,"user_tz":240,"elapsed":32609,"user":{"displayName":"Mark Lee","userId":"14543298645444410514"}},"outputId":"6db8025c-7f77-4a9d-fce0-a18e9b89b35d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training the KNN Classifier!\n","\n","KNN Classifier Accuracy on Training Set = 0.7942152678994784\n","KNN Classifier Accuracy on Test Set = 0.7651515151515151\n","KNN Classifier Sensitivity on Test Set = 0.6\n","KNN Classifier Specificity on Test Set = 0.9027777777777778\n","Optimal value of k = 9\n","Average Runtime of KNN Classifer on Test Set = 0.2588 seconds\n"]}]},{"cell_type":"code","source":["#Train the Logistic Regression Classifier\n","def LogReg_Classifier():\n","  \n","  print(\"Training the Logistic Regression Classifier!\\n\")\n","\n","  #Global parameter variables\n","  l1_ratio = 'none'\n","  penalty = 'l2'\n","  solver = 'lbfgs'\n","  C = 1\n","\n","  #Fine-tune C parameter and penalty/solver choices for maximum accuracy\n","  C_arr = [0.00001, 0.0005, .001, 0.005, 0.01, 0.05, 0.1]\n","  solvers = ['newton-cg', 'lbfgs', 'sag']\n","  penalties = ['l2']\n","\n","  max_val_accuracy = 0.0\n","\n","  #Perform the parameter sweep\n","  for sol in solvers:\n","    for pen in penalties:\n","      for CC in C_arr:\n","        LogReg_clf = LogisticRegression(penalty=pen, C=CC, solver=sol, max_iter=1000)\n","        LogReg_clf.fit(x_train_normal, y_train)\n","\n","        #Compute the classifier accuracy on the validation set\n","        val_accuracy = LogReg_clf.score(x_val_normal, y_val)\n","\n","        if val_accuracy > max_val_accuracy:\n","          C = CC\n","          solver = sol\n","          penalty = pen\n","          max_val_accuracy = val_accuracy\n","\n","  #Repeat the parameter sweep for the 'saga' solver which uses the elastic net penalty\n","  C_arr = [0.00001, 0.0005, .001, 0.005, 0.01, 0.05, 0.1]\n","  solvers = ['saga']\n","  penalties = ['elasticnet', 'l1', 'l2']\n","  l1_ratios = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n","\n","  #Perform the parameter sweep\n","  for sol in solvers:\n","    for pen in penalties:\n","      for CC in C_arr:\n","        for ratio in l1_ratios:\n","          LogReg_clf = LogisticRegression(penalty=pen, C=CC, solver=sol, l1_ratio=ratio, max_iter=1000)\n","          LogReg_clf.fit(x_train_normal, y_train)\n","\n","          #Compute the classifier accuracy on the validation set\n","          val_accuracy = LogReg_clf.score(x_val_normal, y_val)\n","\n","          if val_accuracy > max_val_accuracy:\n","            C = CC\n","            solver = sol\n","            penalty = pen\n","            l1_ratio = ratio\n","            max_val_accuracy = val_accuracy\n","\n","  #Based on the validation set, set the parameter values of the optimal logistic regression algorithm\n","  LogReg_clf = LogisticRegression(penalty=penalty, C=C, solver=solver, l1_ratio=l1_ratio, max_iter=1000)\n","  LogReg_clf.fit(x_train_normal, y_train)\n","  train_accuracy = LogReg_clf.score(x_train_normal, y_train)\n","\n","  #Find the average runtime on the testing set\n","  num_runs = 20\n","  avg_time_LogReg = 0.0\n","  for i in range(0, num_runs):\n","    start_time = time.time()\n","    test_accuracy = LogReg_clf.score(x_test_normal, y_test)\n","    avg_time_LogReg += (time.time() - start_time)\n","  avg_time_LogReg /= num_runs\n","  avg_time_LogReg = str(round(avg_time_LogReg, 4))\n","\n","  #Calculate sensitivity and specificity metrics\n","  y_pred_LogReg = LogReg_clf.predict(x_test_normal)\n","  tn, fp, fn, tp = confusion_matrix(y_test, y_pred_LogReg).ravel()\n","  specificity_LogReg = tn / (tn+fp)\n","  sensitivity_LogReg = tp / (tp+fn)\n","\n","  print(\"Logistic Regression Classifier Accuracy on Test Set = \" + str(test_accuracy))\n","  print(\"Logistic Regression Classifier Accuracy on Training Set = \" + str(train_accuracy))\n","  print(\"Logistic Regression Classifier Sensitivity on Test Set = \" + str(sensitivity_LogReg))\n","  print(\"Logistic Regression Classifier Specificity on Test Set = \" + str(specificity_LogReg))\n","  print(\"Parameters:\")\n","  print(\"   C = \" + str(C))\n","  print(\"   Penalty = \" + str(penalty))\n","  print(\"   Solver = \" + str(solver))\n","  print(\"   L1 Ratio = \" + str(l1_ratio))\n","  print(\"Average Runtime of Logistic Regression Classifer on Test Set = \" + str(avg_time_LogReg) + \" seconds\")\n","\n","LogReg_Classifier()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EQrIqeWonQo2","outputId":"fc7c64f0-3bd0-4bd3-c1c4-04517b4df47e","executionInfo":{"status":"ok","timestamp":1651865974764,"user_tz":300,"elapsed":2806975,"user":{"displayName":"Mark Lee","userId":"14543298645444410514"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Training the Logistic Regression Classifier!\n","\n","Logistic Regression Classifier Accuracy on Test Set = 0.7803030303030303\n","Logistic Regression Classifier Accuracy on Training Set = 0.8330962541488858\n","Logistic Regression Classifier Sensitivity on Test Set = 0.7566666666666667\n","Logistic Regression Classifier Specificity on Test Set = 0.8\n","Parameters:\n","   C = 0.1\n","   Penalty = elasticnet\n","   Solver = saga\n","   L1 Ratio = 0\n","Average Runtime of Logistic Regression Classifer on Test Set = 0.002 seconds\n"]}]},{"cell_type":"code","source":["#Train the Support Vector Machine Classifier\n","def SVM_Classifier():\n","  \n","  print(\"Training the SVM Classifier!\\n\")\n","\n","  #Perform a parameter sweep for the kernel type and C value\n","  C_arr = [0.00001, 0.0005, .001, 0.005, 0.01, 0.05, 0.1]\n","  C = 1\n","  kernel = 'rbf'\n","  degree = 'none'\n","  max_val_accuracy = 0.0\n","\n","  #Sweep 1: Test the linear kernel\n","  print(\"Training SVM Classifier with a linear kernel...\")\n","\n","  for C_val in C_arr:\n","      SVM_clf = svm.SVC(C=C_val, kernel='linear')\n","      SVM_clf.fit(x_train_standard, y_train)\n","\n","      #Compute the classifier accuracy on the validation set\n","      val_accuracy = SVM_clf.score(x_val_standard, y_val)\n","\n","      if val_accuracy > max_val_accuracy:\n","        C = C_val\n","        kernel = 'linear'\n","        max_val_accuracy = val_accuracy\n","\n","  #Sweep 2: Test the polynomial kernel\n","  print(\"Training SVM Classifier with a polynomial kernel...\")\n","\n","  for deg in [1,2,3,4,5,6,7,8,9,10]:\n","      print(\"   Training degree = \" + str(deg) + \"...\")\n","      for C_val in C_arr:\n","        SVM_clf = svm.SVC(C=C_val, kernel='poly', degree=deg)\n","        SVM_clf.fit(x_train_standard, y_train)\n","\n","        #Compute the classifier accuracy on the validation set\n","        val_accuracy = SVM_clf.score(x_val_standard, y_val)\n","\n","        if val_accuracy > max_val_accuracy:\n","          C = C_val\n","          kernel = 'poly'\n","          degree = deg\n","          max_val_accuracy = val_accuracy\n","\n","  #Sweep 3: Test the rbf kernel\n","  print(\"Training SVM Classifier with a RBF kernel...\")\n","\n","  for C_val in C_arr:\n","      SVM_clf = svm.SVC(C=C_val, kernel='rbf')\n","      SVM_clf.fit(x_train_standard, y_train)\n","\n","      #Compute the classifier accuracy on the validation set\n","      val_accuracy = SVM_clf.score(x_val_standard, y_val)\n","\n","      if val_accuracy > max_val_accuracy:\n","        C = C_val\n","        kernel = 'rbf'\n","        max_val_accuracy = val_accuracy\n","\n","  #Sweep 4: Test the sigmoid kernel\n","  print(\"Training SVM Classifier with a sigmoid kernel...\")\n","\n","  for C_val in C_arr:\n","      SVM_clf = svm.SVC(C=C_val, kernel='sigmoid')\n","      SVM_clf.fit(x_train_standard, y_train)\n","\n","      #Compute the classifier accuracy on the validation set\n","      val_accuracy = SVM_clf.score(x_val_standard, y_val)\n","\n","      if val_accuracy > max_val_accuracy:\n","        C = C_val\n","        kernel = 'sigmoid'\n","        max_val_accuracy = val_accuracy\n","\n","  #Based on the validation set, set the parameter values of the optimal SVM algorithm\n","  SVM_clf = svm.SVC(C=C, kernel=kernel, degree=deg)\n","  SVM_clf.fit(x_train_standard, y_train)\n","  train_accuracy = SVM_clf.score(x_train_standard, y_train)\n","  test_accuracy = SVM_clf.score(x_test_standard, y_test)\n","  num_SVs = len(SVM_clf.support_vectors_)\n","\n","  #Find the average runtime on the testing set\n","  num_runs = 20\n","  avg_time_SVM = 0.0\n","  for i in range(0, num_runs):\n","    start_time = time.time()\n","    test_accuracy = SVM_clf.score(x_test_standard, y_test)\n","    avg_time_SVM += (time.time() - start_time)\n","  avg_time_SVM /= num_runs\n","  avg_time_SVM = str(round(avg_time_SVM, 4))\n","\n","  #Calculate sensitivity and specificity metrics\n","  y_pred_SVM = SVM_clf.predict(x_test_standard)\n","  tn, fp, fn, tp = confusion_matrix(y_test, y_pred_SVM).ravel()\n","  specificity_SVM = tn / (tn+fp)\n","  sensitivity_SVM = tp / (tp+fn)\n","\n","  print(\"\\n\")\n","  print(\"SVM Classifier Accuracy on Test Set = \" + str(test_accuracy))\n","  print(\"SVM Classifier Accuracy on Training Set = \" + str(train_accuracy))\n","  print(\"SVM Classifier Sensitivity on Test Set = \" + str(sensitivity_SVM))\n","  print(\"SVM Classifier Specificity on Test Set = \" + str(specificity_SVM))\n","  print(\"Number of support vectors = \" + str(num_SVs))\n","  print(\"Parameters:\")\n","  print(\"   C = \" + str(C))\n","  print(\"   Kernel = \" + str(kernel))\n","  print(\"   Degree = \" + str(degree))\n","  print(\"Average Runtime of SVM Classifer on Test Set = \" + str(avg_time_SVM) + \" seconds\")\n","\n","SVM_Classifier()"],"metadata":{"id":"gY3hDO1jnydP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train the Gaussian Naive Bayes Classifier\n","def NB_Classifier():\n","  \n","  print(\"Training the Gaussian Naive Bayes Classifier!\\n\")\n","\n","  NB_clf = GaussianNB()\n","  NB_clf.fit(x_val_train_standard, y_val_train)\n","  train_accuracy = NB_clf.score(x_val_train_standard, y_val_train)\n","\n","  #Find the average runtime on the testing set\n","  num_runs = 20\n","  avg_time_NB = 0.0\n","  for i in range(0, num_runs):\n","    start_time = time.time()\n","    test_accuracy = NB_clf.score(x_test_standard, y_test)\n","    avg_time_NB += (time.time() - start_time)\n","  avg_time_NB /= num_runs\n","  avg_time_NB = str(round(avg_time_NB, 4))\n","\n","  #Calculate sensitivity and specificity metrics\n","  y_pred_NB = NB_clf.predict(x_test_standard)\n","  tn, fp, fn, tp = confusion_matrix(y_test, y_pred_NB).ravel()\n","  specificity_NB = tn / (tn+fp)\n","  sensitivity_NB = tp / (tp+fn)\n","\n","  print(\"Gaussian Naive Bayes Classifier Accuracy on Test Set = \" + str(test_accuracy))\n","  print(\"Gaussian Naive Bayes Classifier Accuracy on Training Set = \" + str(train_accuracy))\n","  print(\"Gaussian Naive Bayes Classifier Sensitivity on Test Set = \" + str(sensitivity_NB))\n","  print(\"Gaussian Naive Bayes Classifier Specificity on Test Set = \" + str(specificity_NB))\n","  print(\"Average Runtime of Naive Bayes Classifer on Test Set = \" + str(avg_time_NB) + \" seconds\")\n","\n","NB_Classifier()"],"metadata":{"id":"bZ9M7sbhnzQE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train the K-Means Classifier\n","def KM_Classifier():\n","\n","  print(\"Training the K-Means Classifier!\\n\")\n","\n","  from sklearn.metrics.cluster import completeness_score\n","  from sklearn.cluster import KMeans\n","\n","  KM_clf = KMeans(n_clusters=2, random_state=0, max_iter=1000)\n","  KM_clf.fit(x_val_train, y_val_train)\n","\n","  #Find the average runtime on the testing set\n","  num_runs = 20\n","  avg_time_KM = 0.0\n","  for i in range(0, num_runs):\n","    start_time = time.time()\n","    y_pred = KM_clf.predict(x_test)\n","    avg_time_KM += (time.time() - start_time)\n","  avg_time_KM /= num_runs\n","  avg_time_KM = str(round(avg_time_KM, 4))\n","\n","  #Manually find the accuracy of the clustering\n","  num_matches = 0\n","  for i in range(len(y_test)):\n","    if y_pred[i] == y_test[i]:\n","      num_matches += 1\n","  KM_accuracy = num_matches / len(y_test)\n","\n","  #Calculate sensitivity and specificity metrics\n","  y_pred_KM = KM_clf.predict(x_test)\n","  tn, fp, fn, tp = confusion_matrix(y_test, y_pred_KM).ravel()\n","  specificity_KM = tn / (tn+fp)\n","  sensitivity_KM = tp / (tp+fn)\n","\n","  print(\"K-Means Clustering Classifier Accuracy on Test Set = \" + str(KM_accuracy))\n","  print(\"K-Means Clustering Classifier Sensitivity on Test Set = \" + str(sensitivity_KM))\n","  print(\"K-Means Clustering Classifier Specificity on Test Set = \" + str(specificity_KM))\n","  print(\"Average Runtime of K-Means Clustering Classifer on Test Set = \" + str(avg_time_KM) + \" seconds\")\n","\n","KM_Classifier()"],"metadata":{"id":"HrwiSQn2n1XE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train the Multi-Layer Perceptron Classifier\n","def MLP_Classifier():\n","\n","  print(\"Training the Multi-Layer Perceptron Classifier!\\n\")\n","\n","  #Optimize parameters to achieve the highest test accuracy\n","  #The relevant parameters we will optimize are:\n","  #   1) The number of neural network layers\n","  #   2) The number of neurons within each layer\n","  #   3) The activation function\n","  #   4) The solver used for weight optimization\n","  #   5) The alpha value used as the L2 penalty (regularization term) parameter\n","  #Perform a sweep on each parameter separately for efficiency\n","\n","  #Declare optimal parameter variables\n","  highest_accuracy = 0.0\n","  hidden_layer_sizes_opt = ()\n","  activation_opt = ''\n","  solver_opt = ''\n","  alpha_opt = 0.0\n","\n","  #Parameter sweep for the number of layers and size of layers\n","  #Keep other layers at default values\n","  #num_neurons = [10, 25, 50, 75, 100, 125, 150, 175, 200] #Values chosen from observing empirical results\n","  num_neurons = [5, 10, 15, 20, 25, 30, 35, 40] #Values chosen from observing empirical results\n","  num_layers = 3\n","  base_tuple = ()\n","  curr_tuple = ()\n","\n","  print(\"Parameter Sweep for Hidden Layer Configuration:\")\n","  for i in range(1, num_layers+1): #Sweep num_layers layers\n","    best_num = 10\n","    best_local_accuracy = 0.0\n","    for j in num_neurons:\n","\n","      curr_tuple = base_tuple + (j,)\n","      clf = MLPClassifier(hidden_layer_sizes=curr_tuple, random_state=1, max_iter=1000).fit(x_train, y_train)\n","      accuracy = clf.score(x_val, y_val)\n","      print(\"   Testing Layer Configuration: \" + str(curr_tuple) + \",  Validation Accuracy: \" + str(accuracy))\n","\n","      if accuracy > best_local_accuracy:\n","        best_num = j\n","        best_local_accuracy = accuracy\n","\n","      if accuracy > highest_accuracy:\n","        hidden_layer_sizes_opt = curr_tuple\n","        highest_accuracy = accuracy\n","    \n","    base_tuple += (best_num,)\n","  print(\"\\n\")\n","\n","  #Parameter sweep for the activation function\n","  print(\"Parameter Sweep for Activation Function:\")\n","  act_funcs = ['identity', 'logistic', 'tanh', 'relu']\n","  best_local_accuracy = 0.0\n","  for act in act_funcs:\n","    clf = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes_opt, activation=act, random_state=1, max_iter=1000).fit(x_train, y_train)\n","    accuracy = clf.score(x_val, y_val)\n","    print(\"   Testing Activation Function: \" + act + \",  Validation Accuracy: \" + str(accuracy))\n","\n","    if accuracy > best_local_accuracy:\n","      activation_opt = act\n","      best_local_accuracy = accuracy\n","\n","    if accuracy > highest_accuracy:\n","      activation_opt = act\n","      highest_accuracy = accuracy\n","  print(\"\\n\")\n","\n","  #Parameter sweep for the solver\n","  print(\"Parameter Sweep for Solver:\")\n","  solvers = ['lbfgs', 'sgd', 'adam']\n","  best_local_accuracy = 0.0\n","  for sol in solvers:\n","    clf = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes_opt, activation=activation_opt, solver=sol, random_state=1, max_iter=1000).fit(x_train, y_train)\n","    accuracy = clf.score(x_val, y_val)\n","    print(\"   Testing Solver: \" + sol + \",  Validation Accuracy: \" + str(accuracy))\n","\n","    if accuracy > best_local_accuracy:\n","      solver_opt = sol\n","      best_local_accuracy = accuracy\n","\n","    if accuracy > highest_accuracy:\n","      solver_opt = sol\n","      highest_accuracy = accuracy\n","  print(\"\\n\")\n","\n","  #Parameter sweep for alpha\n","  print(\"Parameter Sweep for Alpha:\")\n","  alphas = [0.000001, 0.000005, 0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01]\n","  best_local_accuracy = 0.0\n","  for alpha in alphas:\n","    clf = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes_opt, activation=activation_opt, solver=solver_opt, alpha=alpha, random_state=1, max_iter=1000).fit(x_train, y_train)\n","    accuracy = clf.score(x_val, y_val)\n","    print(\"   Testing Alpha: \" + str(alpha) + \",  Validation Accuracy: \" + str(accuracy))\n","\n","    if accuracy > best_local_accuracy:\n","      alpha_opt = alpha\n","      best_local_accuracy = accuracy\n","\n","    if accuracy > highest_accuracy:\n","      alpha_opt = alpha\n","      highest_accuracy = accuracy\n","  print(\"\\n\")\n","\n","  # Fit MLPClassifier to the training data using the optimal parameters found previously\n","  clf = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes_opt, activation=activation_opt, solver=solver_opt, alpha=alpha_opt, random_state=1, max_iter=1000).fit(x_train, y_train)\n","\n","  # Compute the accuracy on the training set and test set\n","  train_accuracy = clf.score(x_train, y_train)\n","\n","  #Find the average runtime on the testing set\n","  num_runs = 20\n","  avg_time_MLP = 0.0\n","  for i in range(0, num_runs):\n","    start_time = time.time()\n","    test_accuracy = clf.score(x_test, y_test)\n","    avg_time_MLP += (time.time() - start_time)\n","  avg_time_MLP /= num_runs\n","  avg_time_MLP = str(round(avg_time_MLP, 4))\n","\n","  #Calculate sensitivity and specificity metrics\n","  y_pred_MLP = clf.predict(x_test)\n","  tn, fp, fn, tp = confusion_matrix(y_test, y_pred_MLP).ravel()\n","  specificity_MLP = tn / (tn+fp)\n","  sensitivity_MLP = tp / (tp+fn)\n","\n","  print(\"Multi-Layer Perceptron Classifier Accuracy on Test Set = \" + str(test_accuracy))\n","  print(\"Multi-Layer Perceptron Classifier Accuracy on Training Set = \" + str(train_accuracy))\n","  print(\"Multi-Layer Perceptron Classifier Sensitivity on Test Set = \" + str(sensitivity_MLP))\n","  print(\"Multi-Layer Perceptron Classifier Specificity on Test Set = \" + str(specificity_MLP))\n","  print(\"Optimal Parameters:\")\n","  print(\"   Hidden Layer Configuration: \" + str(hidden_layer_sizes_opt))\n","  print(\"      Number of Layers: \" + str(len(hidden_layer_sizes_opt)))\n","  for i in range(1, len(hidden_layer_sizes_opt)+1):\n","    print(\"      Layer \" + str(i) + \" Size: \" + str(hidden_layer_sizes_opt[i-1]) + \" Neurons\")\n","  print(\"   Activation Function: \" + str(activation_opt))\n","  print(\"   Solver for Weight Optimization: \" + str(solver_opt))\n","  print(\"   Alpha: \" + str(alpha_opt))\n","  print(\"Average Runtime of Multi-Layer Perceptron Classifer on Test Set = \" + str(avg_time_MLP) + \" seconds\")\n","\n","MLP_Classifier()"],"metadata":{"id":"F64aF8KAn3L5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train the Decision Tree Classifier\n","def DT_Classifier():\n","\n","  print(\"Training the Decision Tree Classifier!\\n\")\n","\n","  optimal_depth = 4\n","  optimal_leaves = 4\n","\n","  #Parameter sweep for max depth\n","  print(\"Parameter sweep for optimal depth of tree...\")\n","  max_depth = [3,4,5,6,7,8,9,10,11,12,13,14]\n","  acc = 0.0\n","  for i in max_depth:\n","    clf = DecisionTreeClassifier(max_leaf_nodes=optimal_leaves, max_depth=i)\n","    clf = clf.fit(x_train, y_train)\n","    y_pred_val = clf.predict(x_val)\n","    acc_val = accuracy_score(y_val, y_pred_val)\n","    if acc_val > acc:\n","      optimal_depth = i\n","    print('  Accuracy in validation set:', acc_val, 'max depth:', i)\n","  print(\"\\n\")\n","\n","  #Parameter sweep for max leaf nodes\n","  print(\"Parameter sweep for optimal leaf nodes in the tree...\")\n","  max_leaves = [3,4,5,6,7,8,9,10]\n","  acc = 0.0\n","  for i in max_leaves:\n","    clf = DecisionTreeClassifier(max_leaf_nodes=i, max_depth=optimal_depth)\n","    clf = clf.fit(x_train, y_train)\n","    y_pred_val = clf.predict(x_val)\n","    acc_val = accuracy_score(y_val, y_pred_val)\n","    if acc_val > acc:\n","      optimal_leaves = i\n","    print('  Accuracy in validation set:', acc_val, 'max leaf nodes:', i)\n","  print(\"\\n\")\n","\n","  clf = DecisionTreeClassifier(max_leaf_nodes=optimal_leaves, max_depth=optimal_depth)\n","  clf = clf.fit(x_train, y_train)\n","\n","  # Compute the accuracy on the training set and test set\n","  train_accuracy = clf.score(x_train, y_train)\n","\n","  #Find the average runtime on the testing set\n","  num_runs = 50\n","  avg_time_DT = 0.0\n","  for i in range(0, num_runs):\n","    start_time = time.time()\n","    test_accuracy = clf.score(x_test, y_test)\n","    avg_time_DT += (time.time() - start_time)\n","  avg_time_DT /= num_runs\n","  avg_time_DT = str(round(avg_time_DT, 4))\n","\n","  #Calculate sensitivity and specificity metrics\n","  y_pred_DT = clf.predict(x_test)\n","  tn, fp, fn, tp = confusion_matrix(y_test, y_pred_DT).ravel()\n","  specificity_DT = tn / (tn+fp)\n","  sensitivity_DT = tp / (tp+fn)\n","\n","  print(\"Decision Tree Classifier Accuracy on Test Set = \" + str(test_accuracy))\n","  print(\"Decision Tree Classifier Accuracy on Training Set = \" + str(train_accuracy))\n","  print(\"Decision Tree Classifier Sensitivity on Test Set = \" + str(sensitivity_DT))\n","  print(\"Decision Tree Classifier Specificity on Test Set = \" + str(specificity_DT))\n","  print(\"Max Depth of Decision Tree Classifier = \" + str(optimal_depth))\n","  print(\"Max Leaves in Decision Tree Classifier = \" + str(optimal_leaves))\n","  print(\"Average Runtime of Decision Tree Classifer on Test Set = \" + str(avg_time_DT) + \" seconds\")\n","\n","DT_Classifier()"],"metadata":{"id":"teqbaE85n5S8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train the Random Forests Classifier\n","def RF_Classifier():\n","\n","  print(\"Training the Random Forests Classifier!\\n\")\n","\n","  n_estimators_list = [50,80,120,160,200]\n","  criterion_list = ['gini', 'entropy']\n","  max_features_list = ['auto', 'sqrt', 'log2']\n","  max_depth_list = [5, 7, 9]\n","  max_leaf_nodes_list = [10]\n","  params_grid = {\n","      'n_estimators': n_estimators_list,\n","      'criterion': criterion_list,\n","      'max_features': max_features_list,\n","      'max_depth': max_depth_list,\n","      'max_leaf_nodes': max_leaf_nodes_list\n","  }\n","\n","  def my_roc_auc_score(model, X, y): return metrics.roc_auc_score(y, model.predict(X))\n","  num_combinations = 1\n","  for k in params_grid.keys(): num_combinations *= len(params_grid[k])\n","\n","  print('Number of combinations = ', num_combinations)\n","\n","  #params_grid\n","  model_rf = GridSearchCV(estimator=RandomForestClassifier(),\n","                          param_grid=params_grid,\n","                          cv=3,\n","                          scoring=my_roc_auc_score,\n","                          return_train_score=True,\n","                          verbose=4)\n","\n","  model_rf.fit(x_val_train, y_val_train)\n","\n","  best_params = model_rf.best_params_\n","  print('Random Forest Classifier Training Complete! Best parameters are:', best_params)\n","  print(\"\\n\")\n","\n","  best_criterion = best_params['criterion']\n","  best_depth = best_params['max_depth']\n","  best_features = best_params['max_features']\n","  best_nodes = best_params['max_leaf_nodes']\n","  best_estimators = best_params['n_estimators']\n","\n","  clf = RandomForestClassifier(max_leaf_nodes=best_nodes, max_depth=best_depth, n_estimators=best_estimators, criterion=best_criterion, max_features=best_features)\n","  clf = clf.fit(x_val_train, y_val_train)\n","\n","  # Compute the accuracy on the training set and test set\n","  train_accuracy = clf.score(x_val_train, y_val_train)\n","\n","  #Find the average runtime on the testing set\n","  num_runs = 50\n","  avg_time_RF = 0.0\n","  for i in range(0, num_runs):\n","    start_time = time.time()\n","    test_accuracy = clf.score(x_test, y_test)\n","    avg_time_RF += (time.time() - start_time)\n","  avg_time_RF /= num_runs\n","  avg_time_RF = str(round(avg_time_RF, 4))\n","\n","  #Calculate sensitivity and specificity metrics\n","  y_pred_RF = clf.predict(x_test)\n","  tn, fp, fn, tp = confusion_matrix(y_test, y_pred_RF).ravel()\n","  specificity_RF = tn / (tn+fp)\n","  sensitivity_RF = tp / (tp+fn)\n","\n","  print(\"Random Forest Classifier Accuracy on Test Set = \" + str(test_accuracy))\n","  print(\"Random Forest Classifier Accuracy on Training Set = \" + str(train_accuracy))\n","  print(\"Random Forest Classifier Sensitivity on Test Set = \" + str(sensitivity_RF))\n","  print(\"Random Forest Classifier Specificity on Test Set = \" + str(specificity_RF))\n","  print(\"Average Runtime of Random Forest Classifer on Test Set = \" + str(avg_time_RF) + \" seconds\")\n","\n","RF_Classifier()"],"metadata":{"id":"EVCSe8I3n7KS"},"execution_count":null,"outputs":[]}]}